
#include        <cstring> //memset
#include "Parallel.hpp"
#include "blas.h"

#define MIN(X, Y) (((X) < (Y)) ? (X) : (Y))


/***********************************
 *                                 *
 *         DMatrix_dgemm1          *
 *                                 *
 ***********************************/

 /*
  Description:
    This function performs a distributed matrix multiplication using the DGEMM algorithm with additional parallelism
    through MPI communication. It calculates the product of matrices A and B and accumulates the result into matrix C,
    considering the specified block size and process grid configuration. The function manages data distribution and
    communication to perform the multiplication efficiently.

  Parameters:
    - m, n, k: Dimensions of the matrices involved in the multiplication.
    - nblock: Block size for matrix multiplication.
    - alpha: Scalar constant for the multiplication operation.
    - A: Pointer to the input matrix A.
    - lda: Leading dimension of matrix A.
    - ma, na: Pointers to arrays specifying row and column dimensions of submatrices for each process.
    - B: Pointer to the input matrix B.
    - ldb: Leading dimension of matrix B.
    - mb, nb: Pointers to arrays specifying row and column dimensions of submatrices for each process.
    - beta: Scalar constant for scaling matrix C.
    - C: Pointer to the output matrix C, where the result is accumulated.
    - ldc: Leading dimension of matrix C.
    - mc, nc: Pointers to arrays specifying row and column dimensions of result submatrices for each process.
    - taskid_i, taskid_j: Task IDs within the process grid.
    - np_i, np_j: Number of processes in each dimension.
    - comm_i, comm_j: MPI communicators for process rows and columns.
    - work1: Temporary workspace array.
    - work2: Temporary workspace array.
*/

static void DMatrix_dgemm1(Parallel *parall, myarint m, int n, int k, int nblock,
                    double alpha,
                    double *A, int lda, int *ma, int *na,
                    double *B, int ldb, int *mb, int *nb,
                    double beta,
                    double *C, int ldc, int *mc, int *nc,
                    int  taskid_i,int taskid_j,
                    int  np_i, int np_j,
                    MPI_Comm comm_i, MPI_Comm comm_j,
                    double  *work1, double *work2)
{
   int one=1;
   int nn=(nc[taskid_j]*mc[taskid_i]); 
   DSCAL_PWDFT(nn,beta,C,one);
   //for (auto ij=0; ij<(nc[taskid_j]*mc[taskid_i]); ++ij)
   //   C[ij] = beta*C[ij];

   int iwrk,shift,ierr;
   int ii=0;
   int jj=0;
   int kk=0;
   int icur=0;
   int jcur=0;
   double rone=1.0;

   /* loop over all row pannels of C */
   while (kk<k)
   {
      iwrk = MIN(nblock, mb[icur]-ii);
      iwrk = MIN(iwrk,   na[jcur]-jj);
     
      /* pack current iwrk columns of A into work1 */
      if (taskid_j==jcur)
         DLACPY_PWDFT((char*) "G",ma[taskid_i],iwrk,
                      &A[jj*lda],lda,
                      work1, ma[taskid_i]);

      /* pack current iwrk rows of B into work2 */
      if (taskid_i==icur)
         DLACPY_PWDFT((char*) "G",iwrk,nb[taskid_j],
                      &B[ii],ldb,
                      work2, iwrk);


      /* broadcast work1  within my row */
      ierr = MPI_Bcast(work1,iwrk*ma[taskid_i],MPI_DOUBLE_PRECISION,jcur,comm_j);

      /* broadcast work2  within my column */
      ierr = MPI_Bcast(work2,iwrk*nb[taskid_j],MPI_DOUBLE_PRECISION,icur,comm_i);

      if ((iwrk>0) && (mc[taskid_i]>0) &&  (nc[taskid_j]>0))
         d3db::mygdevice.NN_dgemm(mc[taskid_i],nc[taskid_j],iwrk,
                                  alpha,
                                  work1,ma[taskid_i],
                                  work2,iwrk,
                                  rone,
                                  C,ldc);

         /* DGEMM_PWDFT((char *) "N",(char *) "N",mc[taskid_i],nc[taskid_j],iwrk,
                   alpha,
                   work1,ma[taskid_i],
                   work2,iwrk,
                   rone,
                   C,ldc);
         */

      ii += iwrk;
      jj += iwrk;
      kk += iwrk;

      if (jj>=na[jcur]) 
      {
        ++jcur;
        jj=0;
      }
      if (ii>=mc[icur]) 
      {
        ++icur;
        ii=0;
      }

   }
}


/***********************************
 *                                 *
 *         DMatrix_dgemm2          *
 *                                 *
 ***********************************/
 /*
  Description:
    This function performs a distributed matrix multiplication with additional parallelism through MPI communication.
    It calculates the product of matrices transpose(A) and B and accumulates the result into matrix C. The operation is block-wise
    and involves managing data distribution and communication for efficient computation.

  Parameters:
    - m, n, k: Dimensions of the matrices involved in multiplication.
    - nblock: Block size for matrix multiplication.
    - alpha: Scalar constant for the multiplication operation.
    - A: Pointer to the input matrix A.
    - lda: Leading dimension of matrix A.
    - ma, na: Pointers to arrays specifying row and column dimensions of submatrices for each process.
    - B: Pointer to the input matrix B.
    - ldb: Leading dimension of matrix B.
    - mb, nb: Pointers to arrays specifying row and column dimensions of submatrices for each process.
    - beta: Scalar constant for scaling matrix C.
    - C: Pointer to the output matrix C where the result is accumulated.
    - ldc: Leading dimension of matrix C.
    - mc, nc: Pointers to arrays specifying row and column dimensions of result submatrices for each process.
    - taskid_i, taskid_j: Task IDs within the process grid.
    - np_i, np_j: Number of processes in each dimension.
    - comm_i, comm_j: MPI communicators for process rows and columns.
    - work1: Temporary workspace array.
    - work2: Temporary workspace array.
*/

void DMatrix_dgemm2(int m, int n, int k, int nblock,
                    double alpha,
                    double *A, int lda, int *ma, int *na,
                    double *B, int ldb, int *mb, int *nb,
                    double beta,
                    double *C, int ldc, int *mc, int *nc,
                    int  taskid_i,int taskid_j,
                    int  np_i, int np_j,
                    MPI_Comm comm_i, MPI_Comm comm_j,
                    double  *work1, double *work2)
{

   int one=1;
   int nn=(nc[taskid_j]*mc[taskid_i]); 
   DSCAL_PWDFT(nn,beta,C,one);
   //for (auto ij=0; ij<(nc[taskid_j]*mc[taskid_i]); ++ij)
   //   C[ij] = beta*C[ij];

   int iwrk,shift,ierr;
   int ii=0;
   int jj=0;
   int kk=0;
   int icur=0;
   int jcur=0;
   double rone=1.0;
   double rzero=0.0;

   /* loop over all row pannels of C */
   while (kk<m) 
   {
      iwrk = MIN(nblock, mc[icur]-ii);
      iwrk = MIN(iwrk,   na[jcur]-jj);

      /* iwrk*nc(taskid_j) submatrix !=0 */
      if (ma[taskid_i]>0)
      {
         /* pack current iwrk columns of A into work1 */
         if (taskid_j==jcur)
         {
            DLACPY_PWDFT((char*) "G",ma[taskid_i],iwrk,
                         &A[jj*lda],lda,
                         work1, ma[taskid_i]);
         }

         /* broadcast work1  within my row */
         ierr = MPI_Bcast(work1,iwrk*ma[taskid_i],MPI_DOUBLE_PRECISION,jcur,comm_j);

         if ((iwrk>0) && (nb[taskid_j]>0))
            DGEMM_PWDFT((char *) "T",(char *) "N",iwrk,nb[taskid_j],ma[taskid_i],
                   alpha,
                   work1, ma[taskid_i],
                   B, ldb,
                   rzero,
                   work2, iwrk);
      }

      /* iwrk*nc(taskid_j+1) submatrix ==0 */
      else
         memset(work2, 0, nc[taskid_j]*iwrk*sizeof(double));

      /* summ to node that holds current rows of C */
      ierr = MPI_Reduce(work2,work1,nc[taskid_j]*iwrk,
                          MPI_DOUBLE_PRECISION,MPI_SUM,icur,comm_i);

      /* add to current rows of C */
      if (taskid_i==icur) 
      {
         shift = 0;
         for (auto i=ii; i<(ii+iwrk); ++ i)
         {
            DAXPY_PWDFT(nc[taskid_j],rone,&work1[shift],iwrk,&C[i],mc[taskid_i]);
            ++shift;
         }
      }
      ii += iwrk;
      jj += iwrk;
      kk += iwrk;

      if (jj>=na[jcur]) 
      {
        ++jcur;
        jj=0;
      }
      if (ii>=mc[icur]) 
      {
        ++icur;
        ii=0;
      }

   }
}


/***********************************
 *                                 *
 *         DMatrix_dgemm3          *
 *                                 *
 ***********************************/
/*
  Description:
    This function performs a distributed matrix multiplication with additional parallelism through MPI communication.
    It calculates the product of matrices A and transpose(B) and accumulates the result into matrix C. The operation is block-wise
    and involves managing data distribution and communication for efficient computation.
    
  Parameters:
    - m, n, k: Dimensions of the matrices involved in multiplication.
    - nblock: Block size for matrix multiplication.
    - alpha: Scalar constant for the multiplication operation.
    - A: Pointer to the input matrix A.
    - lda: Leading dimension of matrix A.
    - ma, na: Pointers to arrays specifying row and column dimensions of submatrices for each process.
    - B: Pointer to the input matrix B.
    - ldb: Leading dimension of matrix B.
    - mb, nb: Pointers to arrays specifying row and column dimensions of submatrices for each process.
    - beta: Scalar constant for scaling matrix C.
    - C: Pointer to the output matrix C where the result is accumulated.
    - ldc: Leading dimension of matrix C.
    - mc, nc: Pointers to arrays specifying row and column dimensions of result submatrices for each process.
    - taskid_i, taskid_j: Task IDs within the process grid.
    - np_i, np_j: Number of processes in each dimension.
    - comm_i, comm_j: MPI communicators for process rows and columns.
    - work1: Temporary workspace array.
    - work2: Temporary workspace array.
*/

void DMatrix_dgemm3(int m, int n, int k, int nblock,
                    double alpha,
                    double *A, int lda, int *ma, int *na,
                    double *B, int ldb, int *mb, int *nb,
                    double beta,
                    double *C, int ldc, int *mc, int *nc,
                    int  taskid_i,int taskid_j,
                    int  np_i, int np_j,
                    MPI_Comm comm_i, MPI_Comm comm_j,
                    double  *work1, double *work2)
{

   int one=1;
   int nn=(nc[taskid_j]*mc[taskid_i]); 
   DSCAL_PWDFT(nn,beta,C,one);
   //for (auto ij=0; ij<(nc[taskid_j]*mc[taskid_i]); ++ij)
   //   C[ij] = beta*C[ij];

   int iwrk,shift,ierr;
   int ii=0;
   int jj=0;
   int kk=0;
   int icur=0;
   int jcur=0;
   int one = 1;
   double rone=1.0;
   double rzero=0.0;

   /* loop over all row pannels of C */
   while (kk<n)
   {
      iwrk = MIN(nblock, mb[icur]-ii);
      iwrk = MIN(iwrk,   nc[jcur]-jj);
     
      /* pack current iwrk rows of B into work2 */
      if (taskid_i==icur)
         DLACPY_PWDFT((char*) "G",iwrk,nb[taskid_j],
                      &B[ii],ldb,
                      work2, iwrk);

      /* broadcast work2  within my column */
      ierr = MPI_Bcast(work2,iwrk*nb[taskid_j],MPI_DOUBLE_PRECISION,icur,comm_i);

      if ((iwrk>0) && (na[taskid_i]>0) &&  (mc[taskid_j]>0))
         DGEMM_PWDFT((char *) "N",(char *) "T",mc[taskid_i],iwrk,na[taskid_j],
                   alpha,
                   A,lda,
                   work2,iwrk,
                   rzero,
                   work1,mc[taskid_i]);

      /* reduce work1 within my row */
      ierr = MPI_Reduce(work1,work2,mc[taskid_i]*iwrk*,MPI_DOUBLE_PRECISION,jcur,comm_j);

      if (taskid_j==jcur)
      {
         shift = 0;
         for (j=jj; j<(jj+iwrk); ++j)
         {
            DAXPY_PWDFT(mc[taskid_i],rone,&work2[shift],one,&C[j*ldc],one);
            shift += mc[taskid_i];
         }
      }

      ii += iwrk;
      jj += iwrk;
      kk += iwrk;

      if (jj>=nc[jcur]) 
      {
        ++jcur;
        jj=0;
      }
      if (ii>=mb[icur]) 
      {
        ++icur;
        ii=0;
      }

   }
}

